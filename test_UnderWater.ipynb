{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_images\\000021.jpg, detections: 47\n",
      "Processed D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_images\\001481.jpg, detections: 4\n",
      "Processed D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_images\\001484.jpg, detections: 8\n",
      "Processed D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_images\\001485.jpg, detections: 7\n",
      "Processed D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_images\\001486.jpg, detections: 6\n",
      "Processed D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_images\\001488.jpg, detections: 4\n",
      "Processed D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_images\\001489.jpg, detections: 6\n",
      "Processed D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_images\\001492.jpg, detections: 13\n",
      "Processed D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_images\\001495.jpg, detections: 8\n",
      "Processed D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_images\\001499.jpg, detections: 5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from utils.general import non_max_suppression, scale_coords\n",
    "from utils.datasets import letterbox\n",
    "from utils.torch_utils import select_device\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_and_save_results(model, img_path, save_dir, img_size=640, conf_thres=0.25, iou_thres=0.45, device=''):\n",
    "    # Load image\n",
    "    img0 = cv2.imread(img_path)  # BGR\n",
    "    assert img0 is not None, 'Image Not Found ' + img_path\n",
    "\n",
    "    # Padded resize\n",
    "    img = letterbox(img0, img_size, stride=32)[0]\n",
    "\n",
    "    # Convert\n",
    "    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "    img = np.ascontiguousarray(img)\n",
    "\n",
    "    # Load model\n",
    "    if isinstance(device, str):\n",
    "        device = select_device(device)\n",
    "    model.to(device).eval()\n",
    "\n",
    "    # Run inference\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = img.float()  # uint8 to fp16/32\n",
    "    img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "    # Inference\n",
    "    pred = model(img)[0]\n",
    "\n",
    "    # Apply NMS\n",
    "    pred = non_max_suppression(pred, conf_thres, iou_thres, classes=None, agnostic=False)\n",
    "\n",
    "    # Process detections\n",
    "    detections = []\n",
    "    if len(pred):\n",
    "        det = pred[0]\n",
    "        if det is not None and len(det):\n",
    "            # Rescale boxes from img_size to img0 size\n",
    "            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()\n",
    "\n",
    "            # Create save directory if it does not exist\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "            # Save detections to a text file\n",
    "            save_path = os.path.join(save_dir, os.path.splitext(os.path.basename(img_path))[0] + '.txt')\n",
    "            with open(save_path, 'w') as f:\n",
    "                for *xyxy, conf, cls in det:\n",
    "                    xyxy = [coord.item() for coord in xyxy]\n",
    "                    conf = conf.item()\n",
    "                    cls = int(cls.item())\n",
    "                    f.write(f\"{cls} {conf:.4f} \" + \" \".join([str(int(x)) for x in xyxy]) + '\\n')\n",
    "                    detections.append({'class': cls, 'confidence': conf, 'bbox': xyxy})\n",
    "        else:\n",
    "            # Save an empty txt file if no detections\n",
    "            save_path = os.path.join(save_dir, os.path.splitext(os.path.basename(img_path))[0] + '.txt')\n",
    "            open(save_path, 'a').close()\n",
    "            print(f\"No detections for {img_path}, saved empty txt file.\")\n",
    "\n",
    "    return detections\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    weights = 'runs/train/exp12/weights/best.pt'  # path to the model weights\n",
    "    test_dir = r'D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_images'  # path to the directory containing test images\n",
    "    save_dir = r'D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_results'  # directory to save detection results\n",
    "    device = '0'  # GPU device\n",
    "\n",
    "    # Load model\n",
    "    device = select_device(device)\n",
    "    model = torch.load(weights, map_location=device)['model'].float()  # load to FP32\n",
    "    model.to(device).eval()\n",
    "\n",
    "    # Process each image in the test directory\n",
    "    for img_name in os.listdir(test_dir):\n",
    "        img_path = os.path.join(test_dir, img_name)\n",
    "        if img_path.endswith('.jpg') or img_path.endswith('.png'):\n",
    "            detections = detect_and_save_results(model, img_path, save_dir, device=device)\n",
    "            print(f\"Processed {img_path}, detections: {len(detections)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据清理（同一个目标被检测成多个类别，只保留置信度最高的）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_results\\000021.txt\n",
      "Processed D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_results\\001481.txt\n",
      "Processed D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_results\\001484.txt\n",
      "Processed D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_results\\001485.txt\n",
      "Processed D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_results\\001486.txt\n",
      "Processed D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_results\\001488.txt\n",
      "Processed D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_results\\001489.txt\n",
      "Processed D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_results\\001492.txt\n",
      "Processed D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_results\\001495.txt\n",
      "Processed D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_results\\001499.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Function to read detections from .txt file\n",
    "def read_detections_from_txt(txt_path):\n",
    "    detections = []\n",
    "    with open(txt_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 6:\n",
    "                category = int(parts[0])\n",
    "                confidence = float(parts[1])\n",
    "                bbox = list(map(float, parts[2:6]))\n",
    "                detections.append((category, confidence, bbox))\n",
    "    return detections\n",
    "\n",
    "# Function to save detections to .txt file\n",
    "def save_detections_to_txt(detections, txt_path):\n",
    "    with open(txt_path, 'w') as f:\n",
    "        for det in detections:\n",
    "            category, confidence, bbox = det\n",
    "            f.write(f\"{category} {confidence:.4f} \" + \" \".join(map(str, map(int, bbox))) + '\\n')\n",
    "\n",
    "# Function to check if two bounding boxes have coordinates difference less than a threshold\n",
    "def bbox_coordinate_close(bbox1, bbox2, threshold=100):\n",
    "    for i in range(4):\n",
    "        if abs(bbox1[i] - bbox2[i]) > threshold:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Function to clean detection results\n",
    "def clean_detections(results_dir, threshold=100):\n",
    "    for txt_file in os.listdir(results_dir):\n",
    "        if txt_file.endswith('.txt'):\n",
    "            txt_path = os.path.join(results_dir, txt_file)\n",
    "            detections = read_detections_from_txt(txt_path)\n",
    "            \n",
    "            # Initialize a list to keep the cleaned detections\n",
    "            cleaned_detections = []\n",
    "            \n",
    "            # Initialize a list to keep track of which detections have been considered\n",
    "            considered = [False] * len(detections)\n",
    "            \n",
    "            for i in range(len(detections)):\n",
    "                if not considered[i]:\n",
    "                    det1 = detections[i]\n",
    "                    best_det = det1\n",
    "                    for j in range(i + 1, len(detections)):\n",
    "                        if not considered[j]:\n",
    "                            det2 = detections[j]\n",
    "                            if bbox_coordinate_close(det1[2], det2[2], threshold):\n",
    "                                # Mark det2 as considered\n",
    "                                considered[j] = True\n",
    "                                # Keep the detection with the highest confidence\n",
    "                                if det2[1] > best_det[1]:\n",
    "                                    best_det = det2\n",
    "                    # Mark det1 as considered\n",
    "                    considered[i] = True\n",
    "                    # Add the best detection to the cleaned list\n",
    "                    cleaned_detections.append(best_det)\n",
    "            \n",
    "            # Save the cleaned detections back to the file\n",
    "            save_detections_to_txt(cleaned_detections, txt_path)\n",
    "            print(f\"Processed {txt_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results_dir = r'D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_results'\n",
    "    clean_detections(results_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据清理（置信度低于0.5的框删除）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def filter_results(input_directory, confidence_threshold=0.5):\n",
    "    # 遍历指定目录中的所有文件\n",
    "    for filename in os.listdir(input_directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(input_directory, filename)\n",
    "            filtered_data = []\n",
    "\n",
    "            # 读取文件内容\n",
    "            with open(file_path, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "            # 过滤置信度低于阈值的数据\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 2:\n",
    "                    category = parts[0]\n",
    "                    try:\n",
    "                        confidence = float(parts[1])\n",
    "                        if confidence >= confidence_threshold:\n",
    "                            filtered_data.append(line)\n",
    "                    except ValueError:\n",
    "                        # 如果置信度列不是浮点数，则跳过此行\n",
    "                        continue\n",
    "\n",
    "            # 将筛选后的数据写回原文件\n",
    "            with open(file_path, 'w') as file:\n",
    "                file.writelines(filtered_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_directory = r'D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_results'\n",
    "    filter_results(input_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def convert_xml_to_txt(xml_dir, save_dir):\n",
    "    # Create save directory if it does not exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Define the class mapping\n",
    "    class_mapping = {\n",
    "        \"holothurian\": 0,\n",
    "        \"echinus\": 1,\n",
    "        \"scallop\": 2,\n",
    "        \"starfish\": 3\n",
    "    }\n",
    "\n",
    "    for xml_file in os.listdir(xml_dir):\n",
    "        if xml_file.endswith('.xml'):\n",
    "            xml_path = os.path.join(xml_dir, xml_file)\n",
    "            tree = ET.parse(xml_path)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            txt_filename = os.path.splitext(xml_file)[0] + '.txt'\n",
    "            txt_path = os.path.join(save_dir, txt_filename)\n",
    "            \n",
    "            with open(txt_path, 'w') as txt_file:\n",
    "                for obj in root.iter('object'):\n",
    "                    label = obj.find('name').text\n",
    "                    if label not in class_mapping:\n",
    "                        continue  # skip unknown classes\n",
    "                    class_id = class_mapping[label]\n",
    "                    \n",
    "                    bbox = obj.find('bndbox')\n",
    "                    x_min = int(bbox.find('xmin').text)\n",
    "                    y_min = int(bbox.find('ymin').text)\n",
    "                    x_max = int(bbox.find('xmax').text)\n",
    "                    y_max = int(bbox.find('ymax').text)\n",
    "                    \n",
    "                    # Write to txt file in the required format: class_id 0 x_min y_min x_max y_max\n",
    "                    txt_file.write(f\"{class_id} 0 {x_min} {y_min} {x_max} {y_max}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    xml_dir = r'D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_labels'  # path to the directory containing XML files\n",
    "    save_dir = r'D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_labels_txt'  # directory to save converted TXT files\n",
    "\n",
    "    convert_xml_to_txt(xml_dir, save_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检出与实际对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 000021.txt\n",
      "检出: 24, 错检出: 2, 未检出: 6\n",
      "Processed 001481.txt\n",
      "检出: 4, 错检出: 0, 未检出: 1\n",
      "Processed 001484.txt\n",
      "检出: 4, 错检出: 1, 未检出: 0\n",
      "Processed 001485.txt\n",
      "检出: 7, 错检出: 0, 未检出: 2\n",
      "Processed 001486.txt\n",
      "检出: 4, 错检出: 2, 未检出: 1\n",
      "Processed 001488.txt\n",
      "检出: 3, 错检出: 1, 未检出: 0\n",
      "Processed 001489.txt\n",
      "检出: 2, 错检出: 2, 未检出: 5\n",
      "Processed 001492.txt\n",
      "检出: 8, 错检出: 2, 未检出: 1\n",
      "Processed 001495.txt\n",
      "检出: 5, 错检出: 2, 未检出: 0\n",
      "Processed 001499.txt\n",
      "检出: 5, 错检出: 0, 未检出: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Function to read detections from .txt file\n",
    "def read_detections_from_txt(txt_path):\n",
    "    detections = []\n",
    "    with open(txt_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 5:\n",
    "                category = int(parts[0])\n",
    "                confidence = float(parts[1])\n",
    "                bbox = list(map(float, parts[2:6]))\n",
    "                detections.append((category, confidence, bbox))\n",
    "    return detections\n",
    "\n",
    "# Function to check if two bounding boxes have coordinates difference less than a threshold\n",
    "def bbox_coordinate_close(bbox1, bbox2, threshold=100):\n",
    "    for i in range(4):\n",
    "        if abs(bbox1[i] - bbox2[i]) > threshold:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Function to compare detections with ground truth labels and count different situations\n",
    "def compare_detections_with_labels(det_path, label_path):\n",
    "    detections = read_detections_from_txt(det_path)\n",
    "    labels = read_detections_from_txt(label_path)\n",
    "\n",
    "    matched_labels = np.zeros(len(labels), dtype=bool)\n",
    "    detection_counts = {'检出': 0, '错检出': 0, '未检出': len(labels)}\n",
    "    modified_detections = []\n",
    "\n",
    "    for detection in detections:\n",
    "        det_category, _, det_bbox = detection\n",
    "        found_match = False\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            label_category, _, label_bbox = label\n",
    "            if bbox_coordinate_close(det_bbox, label_bbox, threshold=100):\n",
    "                if det_category == label_category:\n",
    "                    matched_labels[i] = True\n",
    "                    found_match = True\n",
    "                    detection_counts['检出'] += 1\n",
    "                    detection_counts['未检出'] -= 1\n",
    "                    modified_detections.append((det_category, 1.0, det_bbox))  # 设置置信度为1\n",
    "                    break\n",
    "                elif not matched_labels[i]:\n",
    "                    found_match = True\n",
    "                    detection_counts['错检出'] += 1\n",
    "                    detection_counts['未检出'] -= 1\n",
    "                    modified_detections.append((det_category, 0.0, det_bbox))  # 设置置信度为0\n",
    "\n",
    "        if not found_match:\n",
    "            detection_counts['错检出'] += 1\n",
    "            modified_detections.append((det_category, 0.0, det_bbox))  # 设置置信度为0\n",
    "\n",
    "    # 对于未检出的标签，添加到 modified_detections 列表中\n",
    "    for i, matched in enumerate(matched_labels):\n",
    "        if not matched:\n",
    "            label_category, _, label_bbox = labels[i]\n",
    "            modified_detections.append((label_category, 0.0, label_bbox))  # 设置置信度为0\n",
    "\n",
    "    return detection_counts, modified_detections\n",
    "\n",
    "# Function to save detections and counts to .txt file\n",
    "def save_detections_and_counts_to_txt(detections, detection_counts, txt_path):\n",
    "    with open(txt_path, 'w') as f:\n",
    "        f.write(f\"检出: {detection_counts['检出']}\\n\")\n",
    "        f.write(f\"错检出: {detection_counts['错检出']}\\n\")\n",
    "        f.write(f\"未检出: {detection_counts['未检出']}\\n\\n\")\n",
    "        for det in detections:\n",
    "            category, confidence, bbox = det\n",
    "            f.write(f\"{category} {confidence:.4f} \" + \" \".join(map(str, map(int, bbox))) + '\\n')\n",
    "\n",
    "# Function to process all images in a directory\n",
    "def process_images(results_dir, labels_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    image_files = os.listdir(results_dir)\n",
    "\n",
    "    for img_file in image_files:\n",
    "        if img_file.endswith('.txt'):\n",
    "            result_path = os.path.join(results_dir, img_file)\n",
    "            label_path = os.path.join(labels_dir, img_file)\n",
    "\n",
    "            if os.path.exists(result_path) and os.path.exists(label_path):\n",
    "                detection_counts, modified_detections = compare_detections_with_labels(result_path, label_path)\n",
    "                output_file = os.path.join(output_dir, img_file)\n",
    "                save_detections_and_counts_to_txt(modified_detections, detection_counts, output_file)\n",
    "                print(f\"Processed {img_file}\")\n",
    "                print(f\"检出: {detection_counts['检出']}, 错检出: {detection_counts['错检出']}, 未检出: {detection_counts['未检出']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results_dir = r'D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_results'\n",
    "    labels_dir = r'D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_labels_txt'\n",
    "    output_dir = r'D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\results_summary'\n",
    "\n",
    "    process_images(results_dir, labels_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def generate_summary_txt(summary_dir, output_file):\n",
    "    detection_counts = {'检出': 0, '错检出': 0, '未检出': 0}\n",
    "\n",
    "    if not os.path.exists(summary_dir):\n",
    "        print(f\"Error: Directory '{summary_dir}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    summary_files = os.listdir(summary_dir)\n",
    "    num_images = 0\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        for file_name in summary_files:\n",
    "            file_path = os.path.join(summary_dir, file_name)\n",
    "            if os.path.isfile(file_path) and file_name.endswith('.txt'):\n",
    "                num_images += 1\n",
    "                with open(file_path, 'r') as img_file:\n",
    "                    lines = img_file.readlines()\n",
    "                    for line in lines:\n",
    "                        if line.startswith('检出'):\n",
    "                            detection_counts['检出'] += int(line.split(':')[1].strip())\n",
    "                        elif line.startswith('错检出'):\n",
    "                            detection_counts['错检出'] += int(line.split(':')[1].strip())\n",
    "                        elif line.startswith('未检出'):\n",
    "                            detection_counts['未检出'] += int(line.split(':')[1].strip())\n",
    "\n",
    "        f.write(f\"检出总数: {detection_counts['检出']}\\n\")\n",
    "        f.write(f\"错检出总数: {detection_counts['错检出']}\\n\")\n",
    "        f.write(f\"未检出总数: {detection_counts['未检出']}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    summary_dir = r'D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\results_summary'\n",
    "    output_file = r'D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\results_summary\\summary.txt'\n",
    "\n",
    "    generate_summary_txt(summary_dir, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际数据总目标数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总行数: 85\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_total_lines(txt_dir):\n",
    "    total_lines = 0\n",
    "\n",
    "    if not os.path.exists(txt_dir):\n",
    "        print(f\"Error: Directory '{txt_dir}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    txt_files = [f for f in os.listdir(txt_dir) if f.endswith('.txt')]\n",
    "\n",
    "    for file_name in txt_files:\n",
    "        file_path = os.path.join(txt_dir, file_name)\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            total_lines += len(lines)\n",
    "\n",
    "    return total_lines\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    txt_dir = r'D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_labels_txt'\n",
    "\n",
    "    total_lines = count_total_lines(txt_dir)\n",
    "    print(f\"总行数: {total_lines}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各类别检测准确率（正确检出总数/所有检出总数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holothurian: 0.4286 (3/7)\n",
      "echinus: 0.6316 (24/38)\n",
      "scallop: 0.6429 (18/28)\n",
      "starfish: 0.8400 (21/25)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Function to read detections from .txt file and extract relevant data\n",
    "def read_detections_from_txt(txt_path):\n",
    "    detections = []\n",
    "    with open(txt_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 6:  # Skip lines that are not detections\n",
    "                category = int(parts[0])\n",
    "                confidence = float(parts[1])\n",
    "                detections.append((category, confidence))\n",
    "    return detections\n",
    "\n",
    "# Function to process all detection files in a directory and calculate proportions\n",
    "def calculate_confidence_proportions(results_dir, output_file):\n",
    "    # Define categories\n",
    "    categories = {\n",
    "        0: 'holothurian',\n",
    "        1: 'echinus',\n",
    "        2: 'scallop',\n",
    "        3: 'starfish'\n",
    "    }\n",
    "    \n",
    "    # Initialize counts for each category\n",
    "    category_counts = {cat: {'total': 0, 'conf_1': 0} for cat in categories.values()}\n",
    "    results = []\n",
    "\n",
    "    image_files = os.listdir(results_dir)\n",
    "    for img_file in image_files:\n",
    "        if img_file.endswith('.txt'):\n",
    "            result_path = os.path.join(results_dir, img_file)\n",
    "            detections = read_detections_from_txt(result_path)\n",
    "\n",
    "            for category, confidence in detections:\n",
    "                if category in categories:\n",
    "                    category_name = categories[category]\n",
    "                    category_counts[category_name]['total'] += 1\n",
    "                    if confidence == 1.0:\n",
    "                        category_counts[category_name]['conf_1'] += 1\n",
    "\n",
    "    # Calculate proportions and print/save results\n",
    "    with open(output_file, 'w') as f:\n",
    "        for category_name, counts in category_counts.items():\n",
    "            total = counts['total']\n",
    "            conf_1 = counts['conf_1']\n",
    "            proportion = conf_1 / total if total > 0 else 0\n",
    "            result = f\"{category_name}: {proportion:.4f} ({conf_1}/{total})\"\n",
    "            results.append(result)\n",
    "            print(result)\n",
    "            f.write(result + '\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results_dir = r'D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\results_summary'\n",
    "    output_file = r'D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\results_summary\\confidence_proportions.txt'\n",
    "    calculate_confidence_proportions(results_dir, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各类别实际准确率（正确检出总数/实际总数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holothurian: 0.5000 (3/6)\n",
      "echinus: 0.7742 (24/31)\n",
      "scallop: 0.7200 (18/25)\n",
      "starfish: 0.9130 (21/23)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Function to read detections from .txt file and extract relevant data\n",
    "def read_detections_from_txt(txt_path):\n",
    "    detections = []\n",
    "    with open(txt_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 6:  # Skip lines that are not detections\n",
    "                category = int(parts[0])\n",
    "                confidence = float(parts[1])\n",
    "                detections.append((category, confidence))\n",
    "    return detections\n",
    "\n",
    "# Function to read labels from .txt file and count each category\n",
    "def count_labels_from_txt(txt_path):\n",
    "    label_counts = {0: 0, 1: 0, 2: 0, 3: 0}\n",
    "    with open(txt_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 1:  # Skip lines that do not have at least one element (category)\n",
    "                category = int(parts[0])\n",
    "                if category in label_counts:\n",
    "                    label_counts[category] += 1\n",
    "    return label_counts\n",
    "\n",
    "# Function to process all detection files in a directory and calculate proportions\n",
    "def calculate_confidence_proportions(results_dir, labels_dir, output_file):\n",
    "    # Define categories\n",
    "    categories = {\n",
    "        0: 'holothurian',\n",
    "        1: 'echinus',\n",
    "        2: 'scallop',\n",
    "        3: 'starfish'\n",
    "    }\n",
    "    \n",
    "    # Initialize counts for each category\n",
    "    category_counts = {cat: {'total': 0, 'conf_1': 0, 'actual': 0} for cat in categories.values()}\n",
    "    results = []\n",
    "\n",
    "    # Count actual labels\n",
    "    label_files = os.listdir(labels_dir)\n",
    "    for label_file in label_files:\n",
    "        if label_file.endswith('.txt'):\n",
    "            label_path = os.path.join(labels_dir, label_file)\n",
    "            label_counts = count_labels_from_txt(label_path)\n",
    "            for category, count in label_counts.items():\n",
    "                if category in categories:\n",
    "                    category_name = categories[category]\n",
    "                    category_counts[category_name]['actual'] += count\n",
    "\n",
    "    # Count detections\n",
    "    image_files = os.listdir(results_dir)\n",
    "    for img_file in image_files:\n",
    "        if img_file.endswith('.txt'):\n",
    "            result_path = os.path.join(results_dir, img_file)\n",
    "            detections = read_detections_from_txt(result_path)\n",
    "\n",
    "            for category, confidence in detections:\n",
    "                if category in categories:\n",
    "                    category_name = categories[category]\n",
    "                    category_counts[category_name]['total'] += 1\n",
    "                    if confidence == 1.0:\n",
    "                        category_counts[category_name]['conf_1'] += 1\n",
    "\n",
    "    # Calculate proportions and print/save results\n",
    "    with open(output_file, 'w') as f:\n",
    "        for category_name, counts in category_counts.items():\n",
    "            actual = counts['actual']\n",
    "            conf_1 = counts['conf_1']\n",
    "            proportion = conf_1 / actual if actual > 0 else 0\n",
    "            result = f\"{category_name}: {proportion:.4f} ({conf_1}/{actual})\"\n",
    "            results.append(result)\n",
    "            print(result)\n",
    "            f.write(result + '\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results_dir = r'D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\results_summary'\n",
    "    labels_dir = r'D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\test_labels_txt'\n",
    "    output_file = r'D:\\Study\\Ai\\sklearn\\Exercise_final\\testimages\\actual_accuracy.txt'\n",
    "    calculate_confidence_proportions(results_dir, labels_dir, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
